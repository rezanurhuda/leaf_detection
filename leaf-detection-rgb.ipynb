{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Import Libraries and Define Directories"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Library\n","import os\n","import re\n","import time\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import matplotlib.pyplot as plt\n","import torch\n","import torchvision\n","from torch.utils.data import DataLoader, Dataset\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","\n","# Define Directories\n","DIR_INPUT = ''\n","DIR_TRAIN = '/kaggle/input/leaf-detection/train'\n","DIR_TEST = '/kaggle/input/leaf-detection/test'\n","device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"]},{"cell_type":"markdown","metadata":{},"source":["# Read and Parse the CSV file"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Reading and parsing the CSV\n","train_df = pd.read_csv(os.path.join(DIR_INPUT, \"/kaggle/input/leaf-detection/train.csv\"))\n","train_df['x'] = -1\n","train_df['y'] = -1\n","train_df['w'] = -1\n","train_df['h'] = -1\n","\n","def expand_bbox(x):\n","    r = np.array(re.findall(\"([0-9]+[.]?[0-9]*)\", x))\n","    if len(r) == 0:\n","        r = [-1, -1, -1, -1]\n","    return r\n","\n","train_df[['x', 'y', 'w', 'h']] = np.stack(train_df['bbox'].apply(lambda x: expand_bbox(x)))\n","train_df.drop(columns=['bbox'], inplace=True)\n","train_df['x'] = train_df['x'].astype(float)\n","train_df['y'] = train_df['y'].astype(float)\n","train_df['w'] = train_df['w'].astype(float)\n","train_df['h'] = train_df['h'].astype(float)\n","image_ids = train_df['image_id'].unique()\n","valid_ids = image_ids[-4:]\n","valid_ids = np.append(valid_ids, image_ids[:4])\n","train_ids = image_ids[4:-4]\n","\n","valid_df = train_df[train_df['image_id'].isin(valid_ids)]\n","train_df = train_df[train_df['image_id'].isin(train_ids)]\n","\n","valid_df.shape, train_df.shape"]},{"cell_type":"markdown","metadata":{},"source":["# Define the Dataset Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class LeafDataset(Dataset):\n","    def __init__(self, dataframe, image_dir, transforms=None):\n","        super().__init__()\n","\n","        self.image_ids = dataframe['image_id'].unique()\n","        self.df = dataframe\n","        self.image_dir = image_dir\n","        self.transforms = transforms\n","\n","    def __getitem__(self, index: int):\n","        image_id = self.image_ids[index]\n","        records = self.df[self.df['image_id'] == image_id]\n","\n","        image = cv2.imread(f'{self.image_dir}/{image_id}', cv2.IMREAD_COLOR)\n","        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)  # Use RGB color space\n","        image /= 255.0\n","\n","        boxes = records[['x', 'y', 'w', 'h']].values.astype(np.float32)  # Ensure boxes are float32\n","        boxes[:, 2] = boxes[:, 0] + boxes[:, 2]\n","        boxes[:, 3] = boxes[:, 1] + boxes[:, 3]\n","\n","        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","        area = torch.as_tensor(area, dtype=torch.float32)\n","\n","        # there is only one class\n","        labels = torch.ones((records.shape[0],), dtype=torch.int64)\n","\n","        # suppose all instances are not crowd\n","        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n","\n","        target = {}\n","        target['boxes'] = torch.as_tensor(boxes, dtype=torch.float32)  # Ensure boxes are float32\n","        target['labels'] = labels\n","        target['image_id'] = torch.tensor([index])\n","        target['area'] = area\n","        target['iscrowd'] = iscrowd\n","\n","        if self.transforms:\n","            sample = {\n","                'image': image,\n","                'bboxes': target['boxes'],\n","                'labels': labels\n","            }\n","            sample = self.transforms(**sample)\n","            image = sample['image']\n","\n","            target['boxes'] = torch.stack(tuple(map(torch.tensor, zip(*sample['bboxes'])))).permute(1, 0)\n","\n","        return image, target, image_id\n","\n","    def __len__(self) -> int:\n","        return self.image_ids.shape[0]"]},{"cell_type":"markdown","metadata":{},"source":["# Define Transformations and DataLoader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This Albumentation for now it is empty.\n","def transform():\n","    return A.Compose([\n","        ToTensorV2(p=1.0),\n","    ], bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n","\n","def collate_fn(batch):\n","    return tuple(zip(*batch))\n","\n","train_dataset = LeafDataset(train_df, DIR_TRAIN, transform())\n","valid_dataset = LeafDataset(valid_df, DIR_TRAIN, transform())\n","\n","train_data_loader = DataLoader(\n","    train_dataset,\n","    batch_size=16,\n","    shuffle=False,\n","    num_workers=4,\n","    collate_fn=collate_fn\n",")\n","\n","valid_data_loader = DataLoader(\n","    valid_dataset,\n","    batch_size=1,\n","    shuffle=False,\n","    num_workers=4,\n","    collate_fn=collate_fn\n",")"]},{"cell_type":"markdown","metadata":{},"source":["# Define the Averager Class"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Averager:\n","    def __init__(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0\n","\n","    def send(self, value):\n","        self.current_total += value\n","        self.iterations += 1\n","\n","    @property\n","    def value(self):\n","        if self.iterations == 0:\n","            return 0\n","        else:\n","            return 1.0 * self.current_total / self.iterations\n","\n","    def reset(self):\n","        self.current_total = 0.0\n","        self.iterations = 0.0"]},{"cell_type":"markdown","metadata":{},"source":["# Define Helper Functions"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# HELPER FUNCTIONS FOR VIZUALISING / PREDICTING\n","\n","def get_boxes(tensor, index, score=0.5):\n","    if index >= len(tensor) or index < 0:\n","        return 0\n","\n","    temp_boxes = []\n","    for i in range(len(tensor[index]['boxes'])):\n","        if tensor[index]['scores'][i] > score:\n","            temp_boxes.append(tensor[index]['boxes'][i].cpu().detach().numpy().astype(np.int32))\n","\n","    return temp_boxes\n","\n","def get_sample_image(itr):\n","    images, targets, image_ids = next(itr)\n","    images = list(image.to(device) for image in images)\n","    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","    boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n","    sample = images[0].permute(1, 2, 0).cpu().numpy()\n","    \n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                      (box[0], box[1]),\n","                      (box[2], box[3]),\n","                      (0, 255, 0), 2)  # Use green color for bounding boxes\n","\n","    ax.set_axis_off()\n","    ax.imshow(sample)\n","\n","def get_validation_image(itr):\n","    images, targets, image_ids = next(itr)\n","    images = list(img.to(device) for img in images)\n","    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","    boxes = targets[0]['boxes'].cpu().numpy().astype(np.int32)\n","    sample = images[0].permute(1, 2, 0).cpu().numpy()\n","\n","    model.eval()\n","\n","    outputs = model(images)\n","    outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","    boxes = get_boxes(outputs, 0)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","\n","    for box in boxes:\n","        cv2.rectangle(sample,\n","                      (box[0], box[1]),\n","                      (box[2], box[3]),\n","                      (0, 255, 0), 2)  # Use green color for bounding boxes\n","\n","    ax.set_axis_off()\n","    ax.imshow(sample)"]},{"cell_type":"markdown","metadata":{},"source":["# Define Test Dataset Loader"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def load_test_dataset():\n","    data_path = DIR_TEST\n","    test_dataset = torchvision.datasets.ImageFolder(\n","        root=data_path,\n","        transform=torchvision.transforms.Compose([\n","            torchvision.transforms.ToTensor(),\n","        ])\n","    )\n","\n","    test_loader = torch.utils.data.DataLoader(\n","        test_dataset,\n","        batch_size=1,\n","        num_workers=1,\n","        shuffle=False\n","    )\n","    return test_loader\n","\n","def get_test_image(itr, score=0.5):\n","    image, targets = next(itr)\n","    sample = image\n","\n","    image = image.to(device)\n","    model.eval()\n","    outputs = model(image)\n","\n","    outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","\n","    boxes = get_boxes(outputs, 0, score)\n","\n","    fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n","    print(sample.shape)\n","    img = sample[0].permute(1, 2, 0).cpu().numpy()\n","    print(img.shape)\n","\n","    img = np.array(img)\n","    print(img.shape)\n","    for box in boxes:\n","        x, y, w, h = box\n","\n","        cv2.rectangle(np.float32(img),\n","                      (int(box[0]), int(box[1])),\n","                      (int(box[2]), int(box[3])),\n","                      (0, 255, 0), 2)  # Use green color for bounding boxes\n","    ax.set_axis_off()\n","    ax.imshow(img)"]},{"cell_type":"markdown","metadata":{},"source":["# Sample Training Data Augmentation and Model Initialization"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Sample of training data augmented\n","\n","it = iter(train_data_loader)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","get_sample_image(it)\n","\n","# Loading ResNet50 trained on COCO\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","\n","num_classes = 2  # 1 class (leaf) + background\n","\n","# get number of input features for the classifier\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","model.to(device)\n","print(\"Model loaded\")"]},{"cell_type":"markdown","metadata":{},"source":["# Define Accuracy Calculation Function"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Function to calculate accuracy\n","def calculate_accuracy(model, data_loader, device):\n","    model.eval()\n","    correct = 0\n","    total = 0\n","    with torch.no_grad():\n","        for images, targets, image_ids in data_loader:\n","            images = list(img.to(device) for img in images)\n","            outputs = model(images)\n","            outputs = [{k: v.to(device) for k, v in t.items()} for t in outputs]\n","\n","            for output, target in zip(outputs, targets):\n","                boxes = output['boxes'].cpu().numpy()\n","                gt_boxes = target['boxes'].cpu().numpy()\n","                if len(boxes) > 0:\n","                    iou_matrix = torchvision.ops.box_iou(torch.tensor(boxes), torch.tensor(gt_boxes))\n","                    max_iou, _ = iou_matrix.max(dim=1)\n","                    correct += (max_iou > 0.5).sum().item()\n","                total += len(gt_boxes)\n","    accuracy = correct / total if total > 0 else 0\n","    return accuracy"]},{"cell_type":"markdown","metadata":{},"source":["# Training Loop"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Training\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n","# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n","lr_scheduler = None\n","\n","num_epochs = 15\n","loss_hist = Averager()\n","itr = 1\n","\n","previous_epoch = 1000\n","es_rate = 0\n","\n","es_threshold = 2 # How many epochs without improvement to early stop\n","\n","for epoch in range(num_epochs):\n","    loss_hist.reset()\n","    min_loss = 1000\n","    \n","    for images, targets, image_ids in train_data_loader:\n","        \n","        images = list(image.to(device) for image in images)\n","        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n","\n","        model.train()\n","        loss_dict = model(images, targets)\n","\n","        losses = sum(loss for loss in loss_dict.values())\n","        loss_value = losses.item()\n","\n","        loss_hist.send(loss_value)\n","\n","        optimizer.zero_grad()\n","        losses.backward()\n","        optimizer.step()\n","        \n","        if itr % 50 == 0:\n","            print(f\"Iteration #{itr} loss: {loss_value}\")\n","        \n","        itr += 1\n","                \n","    # update the learning rate\n","    if lr_scheduler is not None:\n","        lr_scheduler.step()\n","    min_loss = loss_hist.value\n","    \n","    if min_loss < previous_epoch:\n","        previous_epoch = min_loss\n","        es_rate = 0\n","        \n","    else:\n","        if es_rate < es_threshold:\n","            es_rate += 1\n","        elif es_rate >= es_threshold:\n","            break\n","    \n","    # Calculate accuracy on validation set\n","    accuracy = calculate_accuracy(model, valid_data_loader, device)\n","    print(f\"Epoch #{epoch} loss: {loss_hist.value} accuracy: {accuracy}\")"]},{"cell_type":"markdown","metadata":{},"source":["# Validation and Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Validation (On data from Training)\n","it = iter(valid_data_loader)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","get_validation_image(it)\n","\n","# Testing\n","image_list = os.listdir(DIR_TEST + \"/leaf\")\n","print(image_list)\n","it = iter(load_test_dataset())\n","\n","start = time.time()\n","get_test_image(it, 0.5)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.5)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.5)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.5)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.5)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.50)\n","print(time.time() - start)\n","start = time.time()\n","get_test_image(it, 0.50)\n","print(time.time() - start)\n","torch.save(model, 'leaves_fasterrcnn_model.pth')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":707640,"sourceId":1270808,"sourceType":"datasetVersion"}],"dockerImageVersionId":29943,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":4}
